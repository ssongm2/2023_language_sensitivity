{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"oInnPE3xLjD3","outputId":"19e95175-d415-4d54-b202-73df4f2dce0c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting sentencepiece\n","  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.99\n","/bin/bash: -c: line 2: syntax error: unexpected end of file\n","Collecting mxnet\n","  Downloading mxnet-1.9.1-py3-none-manylinux2014_x86_64.whl (49.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.10/dist-packages (from mxnet) (1.23.5)\n","Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.10/dist-packages (from mxnet) (2.31.0)\n","Collecting graphviz<0.9.0,>=0.8.1 (from mxnet)\n","  Downloading graphviz-0.8.4-py2.py3-none-any.whl (16 kB)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet) (2023.7.22)\n","Installing collected packages: graphviz, mxnet\n","  Attempting uninstall: graphviz\n","    Found existing installation: graphviz 0.20.1\n","    Uninstalling graphviz-0.20.1:\n","      Successfully uninstalled graphviz-0.20.1\n","Successfully installed graphviz-0.8.4 mxnet-1.9.1\n","Collecting tensorflow_addons\n","  Downloading tensorflow_addons-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (612 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m612.1/612.1 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow_addons) (23.1)\n","Collecting typeguard<3.0.0,>=2.7 (from tensorflow_addons)\n","  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n","Installing collected packages: typeguard, tensorflow_addons\n","Successfully installed tensorflow_addons-0.21.0 typeguard-2.13.3\n","Collecting emoji\n","  Downloading emoji-2.8.0-py2.py3-none-any.whl (358 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m358.9/358.9 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: emoji\n","Successfully installed emoji-2.8.0\n","Collecting soynlp\n","  Downloading soynlp-0.0.493-py3-none-any.whl (416 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m416.8/416.8 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.12.1 in /usr/local/lib/python3.10/dist-packages (from soynlp) (1.23.5)\n","Requirement already satisfied: psutil>=5.0.1 in /usr/local/lib/python3.10/dist-packages (from soynlp) (5.9.5)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from soynlp) (1.10.1)\n","Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from soynlp) (1.2.2)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->soynlp) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->soynlp) (3.2.0)\n","Installing collected packages: soynlp\n","Successfully installed soynlp-0.0.493\n","Collecting transformers\n","  Downloading transformers-4.32.0-py3-none-any.whl (7.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n","Collecting huggingface-hub<1.0,>=0.15.1 (from transformers)\n","  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m73.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n","  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.7.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n","Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.16.4 safetensors-0.3.3 tokenizers-0.13.3 transformers-4.32.0\n"]}],"source":["!pip install sentencepiece\n","!pip install gluonnlp|\n","!pip install mxnet\n","!pip install tensorflow_addons\n","!pip install emoji\n","!pip install soynlp\n","!pip install transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"txoIHtogSm7x","outputId":"d343e910-5f39-4b94-a9e0-f591131916c1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.32.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.7.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"]}],"source":["!pip install transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"d2PrsaDSSsWn","outputId":"4a9a1c1c-2891-444b-91ed-799be10921e7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting datasets\n","  Downloading datasets-2.14.4-py3-none-any.whl (519 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.3/519.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n","Collecting dill<0.3.8,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting multiprocess (from datasets)\n","  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.16.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.2.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.7.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n","Installing collected packages: xxhash, dill, multiprocess, datasets\n","Successfully installed datasets-2.14.4 dill-0.3.7 multiprocess-0.70.15 xxhash-3.3.0\n"]}],"source":["!pip install datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"DDotyw98N6Kp","outputId":"241c4715-c529-4d71-dfaa-fa0ac96abca8"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n","\n","TensorFlow Addons (TFA) has ended development and introduction of new features.\n","TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n","Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n","\n","For more information see: https://github.com/tensorflow/addons/issues/2807 \n","\n","  warnings.warn(\n"]},{"ename":"ModuleNotFoundError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-8a8f517f7976>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0municodedata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}],"source":["import tensorflow_addons\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import logging\n","import os\n","import unicodedata\n","import utils\n","import time\n","import torch\n","import constants\n","import re\n","import emoji\n","\n","\n","from transformers import PreTrainedTokenizer, TFBertModel, AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding,logging, AdamW\n","from tensorflow_addons.optimizers import RectifiedAdam\n","from keras.models import load_model\n","from shutil import copyfile\n","from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n","from datasets import load_dataset\n","from utils import clean, make_current_datetime_dir, compute_metrics, preprocess_data,multi_label_metrics\n","from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n","from tqdm import tqdm\n","from torch.utils.data import Dataset, DataLoader, TensorDataset\n","from torch.optim.lr_scheduler import ExponentialLR\n","from soynlp.normalizer import repeat_normalize"]},{"cell_type":"markdown","metadata":{"id":"x2CKmELkDcur"},"source":["### Google Mount"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23622,"status":"ok","timestamp":1692841078139,"user":{"displayName":"양시현","userId":"17768312043842105532"},"user_tz":-540},"id":"pCAc1wQ_LqKv","outputId":"b5c5e8be-db51-448b-8ac3-2d48a50b937d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"tSBqkEK3Ooii"},"source":["## Step 1. 감성분석 모델"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vBLOHSQAEJRL"},"outputs":[],"source":["path = '/content/drive/MyDrive/데이터 청년 캠퍼스/모델링/감성분석/Result/8 22 결과'\n","model_a = AutoModelForSequenceClassification.from_pretrained(path)\n","tokenizer_a = AutoTokenizer.from_pretrained(path)\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else 'cpu')\n","model_a = model_a.to(device)  # 모델을 디바이스로 이동"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m5M1yDGVXBOj"},"outputs":[],"source":["def sentimental_predict(sent):\n","    # 모델을 평가 모드로 설정\n","    model_a.eval()\n","\n","    # 문장을 토큰화\n","    tokenized_sent = tokenizer_a(\n","        sent,\n","        return_tensors='pt',\n","        truncation=True,\n","        add_special_tokens=True,\n","        max_length=128\n","    )\n","\n","    # 토큰화된 문장을 디바이스로 이동\n","    tokenized_sent = {k: v.to(device) for k, v in tokenized_sent.items()}\n","\n","    # 모델의 예측 실행\n","    with torch.no_grad():\n","        outputs = model_a(\n","            input_ids=tokenized_sent['input_ids'],\n","            attention_mask=tokenized_sent['attention_mask'],\n","            token_type_ids=tokenized_sent['token_type_ids']\n","        )\n","\n","    # 로짓 추출 및 CPU로 이동\n","    logits = outputs[0].detach().cpu()\n","\n","    # 결과 분류\n","    result = logits.argmax(-1).item()\n","    if result == 0:\n","      print('1. 감성분석의 결과, 이 문장은 부정적인 문장입니다.')\n","    elif result == 1:\n","      print(\"1. 감성분석의 결과, 이 문장은 부정적이지 않은 문장입니다.\")\n"]},{"cell_type":"markdown","metadata":{"id":"IphtILeLOqKO"},"source":["## Step 2: 언어감수성 이진분류 모델"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TgVIdFNcNlYv"},"outputs":[],"source":["path = '/content/drive/MyDrive/데이터 청년 캠퍼스/모델링/언어감수성 판단 유무/Result/언어감수성 판단 유무'\n","model_b = AutoModelForSequenceClassification.from_pretrained(path)\n","tokenizer_b = AutoTokenizer.from_pretrained(path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uKEXduLvPaIi"},"outputs":[],"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else 'cpu')\n","model_b = model_b.to(device)  # 모델을 디바이스로 이동\n","\n","def predict(sent):\n","    # 모델을 평가 모드로 설정\n","    model_b.eval()\n","\n","    # 문장을 토큰화\n","    tokenized_sent = tokenizer_b(\n","        sent,\n","        return_tensors='pt',\n","        truncation=True,\n","        add_special_tokens=True,\n","        max_length=128\n","    )\n","\n","    # 토큰화된 문장을 디바이스로 이동\n","    tokenized_sent = {k: v.to(device) for k, v in tokenized_sent.items()}\n","\n","    # 모델의 예측 실행\n","    with torch.no_grad():\n","        outputs = model_b(\n","            input_ids=tokenized_sent['input_ids'],\n","            attention_mask=tokenized_sent['attention_mask'],\n","            token_type_ids=tokenized_sent['token_type_ids']\n","        )\n","\n","    # 로짓 추출 및 CPU로 이동\n","    logits = outputs[0].detach().cpu()\n","\n","    # 결과 분류\n","    result = logits.argmax(-1).item()\n","    if result == 0:\n","      print('2. 언어감수성 분석 결과, 이 문장은 언어감수성이 높습니다')\n","    elif result == 1:\n","      print(\"2. 언어감수성 분석 결과, 이 문장은 언어감수성이 낮습니다.\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lgs69_xdIOxx","outputId":"8b7fcc5c-e652-4789-e4d4-60e340b01c3e"},"outputs":[{"name":"stdout","output_type":"stream","text":["언어감수성이 낮습니다.\n"]}],"source":["predict(\"오늘 반팔티 입었습니다\")"]},{"cell_type":"markdown","metadata":{"id":"MGoKe2o_QLW1"},"source":["## Step 3: 언어감수성 카테고리 분류 모델"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m2UPfN3CR8Qi"},"outputs":[],"source":["ckpt_path = '/content/drive/MyDrive/데이터 청년 캠퍼스/모델링/언어감수성 카테고리 판단 유무/Result/checkpoint-131560'\n","model_c = AutoModelForSequenceClassification.from_pretrained(ckpt_path).to(device)\n","tokenizer_c = AutoTokenizer.from_pretrained(ckpt_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iyaKqP1zQR3j"},"outputs":[],"source":["def infer(sentences):\n","    global model_c\n","    global tokenizer_c\n","    global device\n","    id2label = constants.ID2LABEL_KOR\n","    results = []\n","\n","    for sentence in sentences:\n","        sentence = clean(sentence)\n","\n","        infer_stime = time.time()\n","        encoding = tokenizer_c(sentence, return_tensors='pt').to(device)\n","        outputs = model_c(**encoding)\n","        logits = outputs.logits\n","        sigmoid = torch.nn.Sigmoid()\n","        preds = sigmoid(logits.squeeze())\n","        infer_etime = time.time()\n","\n","        result = {'문장': sentence,\n","                  '추론시간': infer_etime - infer_stime\n","                  }\n","\n","        for id, label in id2label.items():\n","            prob = preds[id].item()\n","            result[label] = prob\n","\n","        results.append(result)\n","\n","    results = pd.DataFrame(results)\n","    category = results.columns[np.argmax(results.iloc[:, 1:])+1]\n","\n","    return category"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"HTBDnCDVSOM4","outputId":"04c1f6ff-0baa-4022-e93e-d69f59f52d10"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'언어감수성_O'"]},"metadata":{},"output_type":"display_data"}],"source":["sentence = '나는 오늘 너무 행복해'\n","sentences = [sentence]\n","\n","model_c.eval()\n","ret = infer(sentences)\n","display(ret)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"3AnKRnHR79pA"},"source":["## Step 4: 사전에 구축한 대안어 사전 로드"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DX3h8yImLZkL","outputId":"e4db0d97-c101-46c5-819e-536004c5722c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting konlpy\n","  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting JPype1>=0.7.0 (from konlpy)\n","  Downloading JPype1-1.4.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (465 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m465.3/465.3 kB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.3)\n","Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.23.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (23.1)\n","Installing collected packages: JPype1, konlpy\n","Successfully installed JPype1-1.4.1 konlpy-0.6.0\n"]}],"source":["!pip install konlpy\n","import pandas as pd\n","import time\n","\n","from konlpy.tag import Kkma"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vGL5qmEs60Ua"},"outputs":[],"source":["dictionary = pd.read_excel(\"/content/drive/MyDrive/데이터 청년 캠퍼스/사전/5조 챗지피티 애드온 대안어 사전.xlsx\",sheet_name=None)\n","df1 = list(dictionary.values())[0]\n","df1 = df1.drop(\"Unnamed: 0\",axis=1)\n","df2 = list(dictionary.values())[1]\n","df2 = df2[['from','to']]\n","df3 = list(dictionary.values())[2]\n","df3 = df3.drop(\"Unnamed: 0\",axis=1)\n","df4 = list(dictionary.values())[3]\n","df4 = df4.drop(\"Unnamed: 0\",axis=1)\n","df5 = list(dictionary.values())[4]\n","df5 = df5.drop(\"Unnamed: 0\",axis=1)\n","df6 = list(dictionary.values())[5]\n","df6 = df6.drop('ㄱ',axis=1)\n","df6.columns = ['from','to']\n","dfa = pd.concat([df1,df2,df3,df4,df5,df6],axis = 0)\n","dfa.index = range(len(dfa))\n","\n","diction = {i:j for i,j in zip(dfa['from'], dfa['to'])}\n","\n","\n","for i,j in diction.items():\n","  try:\n","    if np.isnan(j):\n","      diction[i] = 'No'\n","  except:\n","    pass"]},{"cell_type":"markdown","metadata":{"id":"dV39i9l48VJZ"},"source":["## Step 5. 대안어 사전 출력 로직"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jbq-xJQd8U84"},"outputs":[],"source":["kkma = Kkma()\n","verb = ['VXV','VCP','VCN','MAG','MAC','IC','JKS','JKC','JKG','JKO','JKM','JKI','JKQ','JX','JC','EPH','EPT','EPP','EFN','EFQ','EFO','EFA','EFI','EFR','ECE','ECD','ECS','ETD','SF','SP','SS','SE','SO','SW','OL','OH','ON']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RKzW20wz9L5x"},"outputs":[],"source":["def Recsys_Sentiment_word_Version_kkma(sentence):\n","  word = set()\n","  pos_sent =kkma.pos(sentence)\n","  verb = ['VXV','VCP','VCN','MAG','MAC','IC','JKS','JKC','JKG','JKO','JKM','JKI','JKQ','JX','JC','EPH','EPT','EPP','EFN','EFQ','EFO','EFA','EFI','EFR','ECE','ECD','ECS','ETD','SF','SP','SS','SE','SO','SW','OL','OH','ON']\n","\n","  ### 1단계 로직 짜기.\n","  First = [i for i in pos_sent if i[1] not in verb]\n","  for word_token in First:\n","    if word_token[0] in list(dfa['from']):\n","      word.add(word_token[0])\n","\n","  ### 2단계 로직 짜기\n","  import nltk\n","\n","  for i in list(nltk.bigrams(First)):\n","    if i[0][0] + i[1][0] in list(dfa['from']):\n","      word.add(i[0][0] + i[1][0])\n","\n","  ### 3단계 로직 짜기\n","  import re\n","\n","  sent_split = re.sub(r'[.,!?(){}[\\]<>%/@;:\"^#*&…~\\-]', '', sentence).split()\n","  for i in pos_sent:\n","    if i[0] in sent_split and i[1] in verb:\n","      sent_split.remove(i[0])\n","  for word_token in sent_split:\n","    if word_token in list(dfa['from']):\n","      word.add(word_token)\n","\n","\n","  ### 마무리 코드 치기\n","  number = 0\n","\n","  for i in word:\n","    if diction[i] != 'No' :\n","      print(\"단어 \\\"{}\\\" 은/는 언어감수성이 낮은 단어로 \\\"{}\\\"로 표현하는 것이 좋아보입니다.\".format(i,diction[i]))\n","      number = 1\n","    else:\n","      print(\"단어 \\\"{}\\\" 은/는 언어감수성이 낮은 단어표현입니다.\".format(i,diction[i]))\n","      number = 1\n","  if number == 0:\n","    print(\"위 문장에는 언어감수성이 낮은 단어가 없습니다.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qHf2O8Qe8fI6","outputId":"6988a3ee-7078-43d6-e9d8-c359f6936659"},"outputs":[{"name":"stdout","output_type":"stream","text":["단어 \"아내\" 은/는 언어감수성이 낮은 단어로 \"배우자\"로 표현하는 것이 좋아보입니다.\n"]}],"source":["Recsys_Sentiment_word_Version_kkma('나는 오늘 아내랑 같이 밥을 먹으러 갈거야')"]},{"cell_type":"markdown","metadata":{"id":"31lrclTI9tzf"},"source":["## Step 6: 사전 자동화 구축 (미완)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OkjRGdg79vbC"},"outputs":[],"source":["def Autodict(sentence):\n","  result = []\n","  noun_sent = kkma.noun(sentence)\n","  for i in noun_sent:\n","    if predict(i) == '언어감수성이 낮습니다':\n","      result.append(i)\n","\n","  return result"]},{"cell_type":"markdown","metadata":{"id":"v5AXPZ4e8Mku"},"source":["### 프로토타입 녹화 부분"]},{"cell_type":"code","source":["sentence = input()\n","\n","### 1. 감성분석 모델\n","start_time = time.time()\n","sentimental_predict(sentence)\n","end_time = time.time()\n","elapsed_time = end_time - start_time\n","print(f\"감성분석 모델 실행 시간: {elapsed_time:.4f}초\")\n","print()\n","\n","### 2. 언어감수성 유무 판단 모델\n","start_time = time.time()\n","predict(sentence)\n","end_time = time.time()\n","elapsed_time = end_time - start_time\n","print(f\"언어감수성 이진분류 모델 실행 시간: {elapsed_time:.4f}초\")\n","print()\n","\n","### 3. 언어감수성 카테고리 판단 모델\n","start_time = time.time()\n","ret = infer([sentence])\n","print(\"3. 언어감수성 카테고리 모델 결과, 이 문장은 \\\"{}\\\" 카테고리에 해당됩니다.\".format(ret))\n","end_time = time.time()\n","elapsed_time = end_time - start_time\n","print(f\"언어감수성 카테고리 모델 코드 실행 시간: {elapsed_time:.4f}초\")\n","print()\n","\n","### 4. 추천 단어 판단\n","print(\"4. 추천 로직 코드 실행\")\n","start_time = time.time()\n","Recsys_Sentiment_word_Version_kkma(sentence)\n","end_time = time.time()\n","elapsed_time = end_time - start_time\n","print(f\"추천 로직 코드 실행 시간: {elapsed_time:.4f}초\")\n","print()\n"],"metadata":{"id":"ebXvGSDsAd3z"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}